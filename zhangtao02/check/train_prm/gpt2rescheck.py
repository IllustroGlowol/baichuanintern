# import json

# # å®šä¹‰æ ‡å‡†æ­¥éª¤æ ‡è®°åˆ—è¡¨
# valid_labels = [
#     "æ­¥éª¤æ­£ç¡®", "å›¾åƒè®¤çŸ¥é”™è¯¯", "é¢˜æ„ç†è§£é”™è¯¯",
#     "ç¼ºä¹ç›¸å…³çŸ¥è¯†", "çŸ¥è¯†åº”ç”¨é”™è¯¯", "é€»è¾‘è¿‡ç¨‹é”™è¯¯",
#     "å¹»è§‰é”™è¯¯", "è¿ç®—å¤„ç†é”™è¯¯", "ä¸æ­¥éª¤ç±»å‹æ ‡ç­¾æ— å…³"
# ]

# # è¾“å…¥è·¯å¾„
# input_path = "/data_train/code/mllm/zhangtao02/check/train_prm/sci50w_llavacot_error.jsonl"
# # è¾“å‡ºè·¯å¾„
# invalid_output_path = "/data_train/code/mllm/zhangtao02/check/train_prm/sci50w_llavacot_error_invalid.jsonl"
# valid_all_output_path = "/data_train/code/mllm/zhangtao02/check/train_prm/sci50w_llavacot_error_valid.jsonl"
# valid_no_unrelated_output_path = "/data_train/code/mllm/zhangtao02/check/train_prm/sci50w_llavacot_error_valid_no_unrelated.jsonl"
# valid_with_unrelated_output_path = "/data_train/code/mllm/zhangtao02/check/train_prm/sci50w_llavacot_error_valid_with_unrelated.jsonl"

# # è®°å½•
# invalid_samples = []
# valid_samples = []
# valid_samples_with_unrelated = []
# valid_samples_no_unrelated = []

# total_count = 0
# invalid_count = 0
# valid_count = 0

# # æ‰“å¼€å¹¶è¯»å–
# with open(input_path, "r", encoding="utf-8") as fin:
#     for line in fin:
#         total_count += 1
#         try:
#             sample = json.loads(line.strip())
#         except Exception as e:
#             print(f"âš ï¸ ç¬¬ {total_count} è¡Œ JSON è§£æå¤±è´¥ï¼Œè·³è¿‡ã€‚é”™è¯¯ï¼š{e}")
#             continue

#         gpt2res = sample.get("gpt2res", None)

#         # ç›´æ¥åˆ¤å®šéæ³•çš„æƒ…å†µ
#         if not isinstance(gpt2res, list):
#             invalid_samples.append(sample)
#             invalid_count += 1
#             continue

#         # æ£€æŸ¥gpt2resä¸­çš„æ¯ä¸ªå­æ­¥éª¤
#         is_invalid = False
#         has_unrelated = False
#         for idx, step in enumerate(gpt2res):
#             if not isinstance(step, list) or len(step) < 3:
#                 is_invalid = True
#                 break
#             label = step[2]
#             if label not in valid_labels:
#                 is_invalid = True
#                 break
#             if label == "ä¸æ­¥éª¤ç±»å‹æ ‡ç­¾æ— å…³":
#                 has_unrelated = True
        
#         if is_invalid:
#             invalid_samples.append(sample)
#             invalid_count += 1
#         else:
#             valid_samples.append(sample)
#             valid_count += 1
#             if has_unrelated:
#                 valid_samples_with_unrelated.append(sample)
#             else:
#                 valid_samples_no_unrelated.append(sample)

# # ä¿å­˜ä¸åˆæ ¼æ ·æœ¬
# if invalid_samples:
#     with open(invalid_output_path, "w", encoding="utf-8") as fout:
#         for sample in invalid_samples:
#             fout.write(json.dumps(sample, ensure_ascii=False) + "\n")

# # ä¿å­˜æ‰€æœ‰åˆæ ¼æ ·æœ¬
# if valid_samples:
#     with open(valid_all_output_path, "w", encoding="utf-8") as fout:
#         for sample in valid_samples:
#             fout.write(json.dumps(sample, ensure_ascii=False) + "\n")

# # ä¿å­˜åˆæ ¼ä¸”æ— "ä¸æ­¥éª¤ç±»å‹æ ‡ç­¾æ— å…³"çš„æ ·æœ¬
# if valid_samples_no_unrelated:
#     with open(valid_no_unrelated_output_path, "w", encoding="utf-8") as fout:
#         for sample in valid_samples_no_unrelated:
#             fout.write(json.dumps(sample, ensure_ascii=False) + "\n")

# # ä¿å­˜åˆæ ¼ä½†åŒ…å«"ä¸æ­¥éª¤ç±»å‹æ ‡ç­¾æ— å…³"çš„æ ·æœ¬
# if valid_samples_with_unrelated:
#     with open(valid_with_unrelated_output_path, "w", encoding="utf-8") as fout:
#         for sample in valid_samples_with_unrelated:
#             fout.write(json.dumps(sample, ensure_ascii=False) + "\n")

# # æ‰“å°æ€»ç»“
# print(f"ğŸ” æ£€æŸ¥å®Œæˆï¼Œæ€»æ ·æœ¬æ•°: {total_count}")
# print(f"âœ… åˆæ ¼æ ·æœ¬æ•°: {valid_count}")
# print(f"âŒ ä¸åˆæ ¼æ ·æœ¬æ•°: {invalid_count}")
# print(f"âœ… åˆæ ¼ä¸”æ— 'ä¸æ­¥éª¤ç±»å‹æ ‡ç­¾æ— å…³'æ ·æœ¬æ•°: {len(valid_samples_no_unrelated)}ï¼Œä¿å­˜åˆ°: {valid_no_unrelated_output_path}")
# print(f"âœ… åˆæ ¼ä½†åŒ…å«'ä¸æ­¥éª¤ç±»å‹æ ‡ç­¾æ— å…³'æ ·æœ¬æ•°: {len(valid_samples_with_unrelated)}ï¼Œä¿å­˜åˆ°: {valid_with_unrelated_output_path}")
# print(f"âœ… æ‰€æœ‰åˆæ ¼æ ·æœ¬ä¿å­˜åˆ°: {valid_all_output_path}")
# print(f"âŒ ä¸åˆæ³•æ ·æœ¬ä¿å­˜åˆ°: {invalid_output_path}")
# import json

# # æ ‡å‡†çš„æ­¥éª¤æ ‡è®°
# valid_labels = [
#     "æ­¥éª¤æ­£ç¡®", "å›¾åƒè®¤çŸ¥é”™è¯¯", "é¢˜æ„ç†è§£é”™è¯¯",
#     "ç¼ºä¹ç›¸å…³çŸ¥è¯†", "çŸ¥è¯†åº”ç”¨é”™è¯¯", "é€»è¾‘è¿‡ç¨‹é”™è¯¯",
#     "å¹»è§‰é”™è¯¯", "è¿ç®—å¤„ç†é”™è¯¯", "ä¸æ­¥éª¤ç±»å‹æ ‡ç­¾æ— å…³"
# ]

# # æ–‡ä»¶è·¯å¾„
# input_path = "/data_train/code/mllm/zhangtao02/check/train_prm/sci50w_perfect.jsonl"
# invalid_output_path = "/data_train/code/mllm/zhangtao02/check/train_prm/sci50w_perfect_invalid.jsonl"
# valid_all_output_path = "/data_train/code/mllm/zhangtao02/check/train_prm/sci50w_perfect_valid.jsonl"
# valid_no_unrelated_output_path = "/data_train/code/mllm/zhangtao02/check/train_prm/sci50w_perfect_valid_no_unrelated.jsonl"
# valid_with_unrelated_output_path = "/data_train/code/mllm/zhangtao02/check/train_prm/sci50w_perfect_valid_with_unrelated.jsonl"

# # è®°å½•
# invalid_samples = []
# valid_samples = []
# valid_samples_no_unrelated = []
# valid_samples_with_unrelated = []

# total_count = 0
# invalid_count = 0
# valid_count = 0

# # æ‰“å¼€æ–‡ä»¶å¼€å§‹è¯»å–
# with open(input_path, "r", encoding="utf-8") as fin:
#     for line in fin:
#         total_count += 1
#         try:
#             sample = json.loads(line.strip())
#         except Exception as e:
#             print(f"âš ï¸ ç¬¬ {total_count} è¡Œ JSON è§£æå¤±è´¥ï¼Œè·³è¿‡ã€‚é”™è¯¯ï¼š{e}")
#             continue

#         gpt2res = sample.get("gpt2res", None)
#         path_nodes = sample.get("path_nodes", None)

#         # ç›´æ¥éæ³•çš„æƒ…å†µ
#         if not isinstance(gpt2res, list) or not isinstance(path_nodes, list):
#             invalid_samples.append(sample)
#             invalid_count += 1
#             continue

#         # æ–°å¢çš„é•¿åº¦è§„åˆ™æ ¡éªŒ
#         expected_len = len(path_nodes) - 1
#         if len(gpt2res) != expected_len:
#             invalid_samples.append(sample)
#             invalid_count += 1
#             continue

#         # æ£€æŸ¥æ¯ä¸ªæ­¥éª¤æ˜¯å¦åˆæ³•
#         is_invalid = False
#         has_unrelated = False
#         for idx, step in enumerate(gpt2res):
#             if not isinstance(step, list) or len(step) < 3:
#                 is_invalid = True
#                 break
#             label = step[1]  # æ³¨æ„ï¼gpt2resè¿™é‡Œæ­¥éª¤æ ‡è®°åœ¨ç¬¬2ä¸ªå…ƒç´ ï¼
#             if label not in valid_labels:
#                 is_invalid = True
#                 break
#             if label == "ä¸æ­¥éª¤ç±»å‹æ ‡ç­¾æ— å…³":
#                 has_unrelated = True

#         if is_invalid:
#             invalid_samples.append(sample)
#             invalid_count += 1
#         else:
#             valid_samples.append(sample)
#             valid_count += 1
#             if has_unrelated:
#                 valid_samples_with_unrelated.append(sample)
#             else:
#                 valid_samples_no_unrelated.append(sample)

# # ä¿å­˜ä¸åˆè§„æ ·æœ¬
# if invalid_samples:
#     with open(invalid_output_path, "w", encoding="utf-8") as fout:
#         for sample in invalid_samples:
#             fout.write(json.dumps(sample, ensure_ascii=False) + "\n")

# # ä¿å­˜æ‰€æœ‰åˆè§„æ ·æœ¬
# if valid_samples:
#     with open(valid_all_output_path, "w", encoding="utf-8") as fout:
#         for sample in valid_samples:
#             fout.write(json.dumps(sample, ensure_ascii=False) + "\n")

# # ä¿å­˜åˆè§„ä¸”æ— "ä¸æ­¥éª¤ç±»å‹æ ‡ç­¾æ— å…³"çš„æ ·æœ¬
# if valid_samples_no_unrelated:
#     with open(valid_no_unrelated_output_path, "w", encoding="utf-8") as fout:
#         for sample in valid_samples_no_unrelated:
#             fout.write(json.dumps(sample, ensure_ascii=False) + "\n")

# # ä¿å­˜åˆè§„ä¸”æœ‰"ä¸æ­¥éª¤ç±»å‹æ ‡ç­¾æ— å…³"çš„æ ·æœ¬
# if valid_samples_with_unrelated:
#     with open(valid_with_unrelated_output_path, "w", encoding="utf-8") as fout:
#         for sample in valid_samples_with_unrelated:
#             fout.write(json.dumps(sample, ensure_ascii=False) + "\n")

# # æ‰“å°æ€»ç»“
# print(f"ğŸ” æ£€æŸ¥å®Œæˆï¼Œæ€»æ ·æœ¬æ•°: {total_count}")
# print(f"âœ… åˆæ ¼æ ·æœ¬æ•°: {valid_count}")
# print(f"âŒ ä¸åˆæ ¼æ ·æœ¬æ•°: {invalid_count}")
# print(f"âœ… åˆæ ¼ä¸”æ— 'ä¸æ­¥éª¤ç±»å‹æ ‡ç­¾æ— å…³'æ ·æœ¬æ•°: {len(valid_samples_no_unrelated)}")
# print(f"âœ… åˆæ ¼ä¸”å«æœ‰'ä¸æ­¥éª¤ç±»å‹æ ‡ç­¾æ— å…³'æ ·æœ¬æ•°: {len(valid_samples_with_unrelated)}")
# print(f"âœ… æ‰€æœ‰åˆæ ¼æ ·æœ¬ä¿å­˜åˆ°: {valid_all_output_path}")
# print(f"âŒ ä¸åˆæ³•æ ·æœ¬ä¿å­˜åˆ°: {invalid_output_path}")



import json
import random

# è¾“å…¥è·¯å¾„
no_unrelated_path = "/data_train/code/mllm/zhangtao02/check/train_prm/sci50w_perfect_valid_no_unrelated.jsonl"
with_unrelated_path = "/data_train/code/mllm/zhangtao02/check/train_prm/sci50w_perfect_valid_with_unrelated.jsonl"

# éœ€è¦æŠ½å–çš„æ•°é‡
sample_num = 30000

# no_unrelated_path = "/data_train/code/mllm/zhangtao02/check/train_prm/sci50w_error_valid_no_unrelated.jsonl"
# with_unrelated_path = "/data_train/code/mllm/zhangtao02/check/train_prm/sci50w_error_valid_with_unrelated.jsonl"
# sample_num = 45000

# è¯»å–æ— unrelatedçš„æ ·æœ¬
no_unrelated_samples = []
with open(no_unrelated_path, "r", encoding="utf-8") as fin:
    for line in fin:
        try:
            sample = json.loads(line.strip())
            no_unrelated_samples.append(sample)
        except Exception as e:
            print(f"âš ï¸ è·³è¿‡æ— æ•ˆè¡Œï¼Œé”™è¯¯: {e}")

# æ£€æŸ¥æ ·æœ¬æ˜¯å¦å¤Ÿ
if len(no_unrelated_samples) < sample_num:
    raise ValueError(f"âŒ æ ·æœ¬æ•°é‡ä¸è¶³ï¼Œåªæ‰¾åˆ° {len(no_unrelated_samples)} æ¡ï¼Œæ— æ³•æŠ½å– {sample_num} æ¡ã€‚")

# éšæœºæŠ½æ ·
sampled_samples = random.sample(no_unrelated_samples, sample_num)

# ç”Ÿæˆå‰©ä½™æ ·æœ¬
sampled_set = set(json.dumps(s, ensure_ascii=False) for s in sampled_samples)
remaining_samples = [s for s in no_unrelated_samples if json.dumps(s, ensure_ascii=False) not in sampled_set]

# è¯»å–å·²æœ‰çš„with_unrelatedæ ·æœ¬
with_unrelated_samples = []
with open(with_unrelated_path, "r", encoding="utf-8") as fin:
    for line in fin:
        try:
            sample = json.loads(line.strip())
            with_unrelated_samples.append(sample)
        except Exception as e:
            print(f"âš ï¸ è·³è¿‡æ— æ•ˆè¡Œï¼Œé”™è¯¯: {e}")

# åˆå¹¶æ–°åŠ è¿›å»
merged_samples = with_unrelated_samples + sampled_samples

# ä¿å­˜æ›´æ–°çš„ with_unrelated æ–‡ä»¶ï¼ˆè¦†ç›–ï¼‰
with open(with_unrelated_path, "w", encoding="utf-8") as fout:
    for sample in merged_samples:
        fout.write(json.dumps(sample, ensure_ascii=False) + "\n")

# ä¿å­˜æ›´æ–°åçš„ no_unrelated æ–‡ä»¶ï¼ˆè¦†ç›–ï¼‰
with open(no_unrelated_path, "w", encoding="utf-8") as fout:
    for sample in remaining_samples:
        fout.write(json.dumps(sample, ensure_ascii=False) + "\n")

# æ‰“å°æ€»ç»“
print(f"âœ… æˆåŠŸä» {no_unrelated_path} æŠ½å– {sample_num} æ¡å¹¶è¿½åŠ åˆ° {with_unrelated_path}")
print(f"âœ… {no_unrelated_path} ä¸­å‰©ä½™ {len(remaining_samples)} æ¡")
print(f"âœ… {with_unrelated_path} ä¸­æ€»è®¡ {len(merged_samples)} æ¡")
